{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Problem"],"metadata":{"id":"PDZxOtCZt5VU"}},{"cell_type":"markdown","source":["$$f(w_1, w_2) = 0.1w_1^2 + 2w_2^2 \\;\\;\\;\\;\\;\\;\\;(1)$$ "],"metadata":{"id":"Z7esFCIXuNXS"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"_IAVg99F9N0y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Adam"],"metadata":{"id":"77XQZxMK2l4x"}},{"cell_type":"code","source":["def df_w(w):\n","    \"\"\"\n","    Thực hiện tính gradient của dw1 và dw2\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    Returns:\n","    dW -- np.array [dw1, dw2], array chứa giá trị đạo hàm theo w1 và w2 \n","    \"\"\"\n","    #################### YOUR CODE HERE ####################\n","    \n","\n","    dW = \n","    ########################################################\n","    \n","    return dW"],"metadata":{"id":"uai1hzbWuNaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Adam(W, dW, lr, V, S, beta1, beta2, t):\n","    \"\"\"\n","    Thực hiện thuật tóan Adam để update w1 và w2\n","    Arguments:\n","    W -- np.array: [w1, w2]\n","    dW -- np.array: [dw1, dw2], array chứa giá trị đạo hàm theo w1 và w2 \n","    lr -- float: learning rate \n","    V -- np.array: [v1, v2] Exponentially weighted averages gradients\n","    S -- np.array: [s1, s2] Exponentially weighted averages bình phương gradients\n","    beta1 -- float: hệ số long-range average cho V\n","    beta2 -- float: hệ số long-range average cho S\n","    t -- int: lần thứ t update (bắt đầu bằng 1)\n","    Returns:\n","    W -- np.array: [w1, w2] w1 và w2 sau khi đã update\n","    V -- np.array: [v1, v2] Exponentially weighted averages gradients sau khi đã cập nhật\n","    S -- np.array: [s1, s2] Exponentially weighted averages bình phương gradients sau khi đã cập nhật\n","    \"\"\"\n","    epsilon = 1e-6\n","    #################### YOUR CODE HERE ####################\n","    V =\n","    S =\n","\n","    W = \n","    ########################################################\n","    return W, V, S"],"metadata":{"id":"d9HnD57lC1X1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_p1(optimizer, lr, epochs):\n","    \"\"\"\n","    Thực hiện tìm điểm minimum của function (1) dựa vào thuật toán \n","    được truyền vào từ optimizer\n","    Arguments:\n","    optimize : function thực hiện thuật toán optimization cụ thể\n","    lr -- float: learning rate \n","    epochs -- int: số lượng lần (epoch) lặp để tìm điểm minimum \n","    Returns:\n","    results -- list: list các cặp điểm [w1, w2] sau mỗi epoch (mỗi lần cập nhật)\n","    \"\"\"\n","    # initial\n","    W = np.array([-5, -2], dtype=np.float32)\n","    V = np.array([0, 0], dtype=np.float32)\n","    S = np.array([0, 0], dtype=np.float32)\n","    results = [W]\n","    #################### YOUR CODE HERE ####################\n","    # Tạo vòng lặp theo số lần epochs\n","    # tìm gradient dW gồm dw1 và dw2\n","    # dùng thuật toán optimization cập nhật w1, w2, s1, s2, v1, v2\n","    # append cặp [w1, w2] vào list results\n","    # các bạn lưu ý mỗi lần lặp nhớ lấy t (lần thứ t lặp) và t bất đầu bằng 1\n","\n","    \n","    ########################################################\n","    return results"],"metadata":{"id":"DSux416mEjYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_p1(Adam, lr=0.2, epochs=30)"],"metadata":{"id":"uwBvH3HeEjUL"},"execution_count":null,"outputs":[]}]}