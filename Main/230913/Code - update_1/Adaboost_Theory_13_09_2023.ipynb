{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfSVnP1wpiYx","executionInfo":{"status":"ok","timestamp":1698845395948,"user_tz":-420,"elapsed":3187,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}},"outputId":"20f68e0c-1447-457a-e050-c153b2d55bb8"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":27,"metadata":{"id":"tLrDgEQIF4ZJ","executionInfo":{"status":"ok","timestamp":1698845395949,"user_tz":-420,"elapsed":11,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}}},"outputs":[],"source":["\n","# Imports\n","import numpy as np\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Helper functions\n","def compute_error(y, y_pred, w_i):\n","    '''\n","    Calculate the error rate of a weak classifier m. Arguments:\n","    y: actual target value\n","    y_pred: predicted value by weak classifier\n","    w_i: individual weights for each observation\n","\n","\n","    Note that all arrays should be the same length\n","    '''\n","\n","    return (sum(w_i * (np.not_equal(y, y_pred)).astype(int)))/sum(w_i)\n","\n","def compute_alpha(error):\n","    '''\n","    Calculate the weight of a weak classifier m in the majority vote of the final classifier. This is called\n","    alpha in chapter 10.1 of The Elements of Statistical Learning. Arguments:\n","    error: error rate from weak classifier m\n","    '''\n","    espilon = 0.01\n","    return np.log((1 - error + espilon) / (error+espilon))\n","\n","def update_weights_formular1(w_i, alpha, y, y_pred):\n","    result = w_i * np.exp(-alpha * y * y_pred)\n","    w_norm = result / np.sum(result)\n","    return w_norm\n","\n","\n","def update_weights_formular2(w_i, alpha, y, y_pred):\n","    result = w_i * np.exp(alpha * (\n","        np.not_equal(y, y_pred)).astype(int))\n","    w_norm = result / np.sum(result)\n","    return w_norm\n","\n","\n","# Define AdaBoost class\n","class AIVNAdaBoost:\n","\n","    def __init__(self):\n","        # self.w_i = None\n","        self.alphas = []\n","        self.G_M = []\n","        self.M = None\n","        self.training_errors = []\n","        self.prediction_errors = []\n","\n","    def fit(self, X, y, M = 100):\n","        '''\n","        Fit model. Arguments:\n","        X: independent variables\n","        y: target variable\n","        M: number of boosting rounds. Default is 100\n","        '''\n","\n","        # Clear before calling\n","        self.alphas = []\n","        self.training_errors = []\n","        self.M = M\n","\n","        # Iterate over M weak classifiers\n","        for m in range(0, M):\n","\n","            # Set weights for current boosting iteration\n","            if m == 0:\n","                w_i = np.ones(len(y)) * 1 / len(y)  # At m = 0, weights are all the same and equal to 1 / N\n","            else:\n","                 w_i = update_weights_formular2(w_i, alpha_m, y, y_pred)\n","                # w_i = update_weights_formular1(w_i, alpha_m, y, y_pred)\n","            # print(w_i)\n","\n","            # (a) Fit weak classifier and predict labels\n","            G_m = DecisionTreeClassifier(max_depth = 1)     # Stump: Two terminal-node classification tree\n","            G_m.fit(X, y, sample_weight = w_i)\n","            y_pred = G_m.predict(X)\n","\n","            self.G_M.append(G_m) # Save to list of weak classifiers\n","\n","            # (b) Compute error\n","            error_m = compute_error(y, y_pred, w_i)\n","            self.training_errors.append(error_m)\n","            # print(error_m)\n","\n","            # (c) Compute alpha\n","            alpha_m = compute_alpha(error_m)\n","            self.alphas.append(alpha_m)\n","            # print(alpha_m)\n","\n","        assert len(self.G_M) == len(self.alphas)\n","\n","\n","    def predict(self, X):\n","        '''\n","        Predict using fitted model. Arguments:\n","        X: independent variables\n","        '''\n","\n","        # Initialise dataframe with weak predictions for each observation\n","        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.M))\n","\n","        # Predict class label for each weak classifier, weighted by alpha_m\n","        for m in range(self.M):\n","            y_pred_m = self.G_M[m].predict(X) * self.alphas[m]\n","            #weak_preds.iloc[:,m] = y_pred_m\n","            weak_preds[weak_preds.columns[m]] = y_pred_m\n","\n","        # Estimate final predictions\n","        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n","\n","        return y_pred\n","\n","    def error_rates(self, X, y):\n","        '''\n","        Get the error rates of each weak classifier. Arguments:\n","        X: independent variables\n","        y: target variables associated to X\n","        '''\n","\n","        self.prediction_errors = [] # Clear before calling\n","\n","        # Predict class label for each weak classifier\n","        for m in range(self.M):\n","            y_pred_m = self.G_M[m].predict(X)\n","            error_m = compute_error(y = y, y_pred = y_pred_m, w_i = np.ones(len(y)))\n","            self.prediction_errors.append(error_m)"]},{"cell_type":"code","source":["result = [0.07, 0.07, 0.07, 0.22, 0.07, 0.07, 0.07, 0.22]\n","w_norm = result / np.sum(result)\n","print(w_norm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGGdVVXqRuqW","outputId":"b4f06f47-08d0-4f79-86fc-ed0f0b53a1f6","executionInfo":{"status":"ok","timestamp":1698845395950,"user_tz":-420,"elapsed":10,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.08139535 0.08139535 0.08139535 0.25581395 0.08139535 0.08139535\n"," 0.08139535 0.25581395]\n"]}]},{"cell_type":"code","source":["# Imports\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import AdaBoostClassifier"],"metadata":{"id":"aEeLX2whGFeP","executionInfo":{"status":"ok","timestamp":1698845395950,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["#Prepare dataset\n","X, y = make_classification(n_samples= 1000, n_features = 20, random_state = 42)\n","y = y * 2 - 1       # Original AdaBoost uses {1, -1} as class labels\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"],"metadata":{"id":"cAE527KaGXVq","executionInfo":{"status":"ok","timestamp":1698845395950,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["#Prepare spam dataset\n","\n","df = pd.read_csv('/content/drive/Othercomputers/My Laptop/Advanced/D-Learning/230913/Code - update_1/spambase.data', header = None)\n","names = pd.read_csv('/content/drive/Othercomputers/My Laptop/Advanced/D-Learning/230913/Code - update_1/spambase.names', sep = ':', skiprows=range(0, 33), header = None)\n","col_names = list(names[0])\n","col_names.append('Spam')\n","df.columns = col_names\n","df.head()\n","\n","df['Spam'] = df['Spam'] * 2 - 1\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = 'Spam').values, df['Spam'].values, test_size = 0.2, random_state = 2)\n","\n","\n"],"metadata":{"id":"AHc7MLadVZ1p","executionInfo":{"status":"ok","timestamp":1698845395951,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Fit model\n","ab = AIVNAdaBoost()\n","ab.fit(X_train, y_train, M = 50)\n","\n","# Predict on test set\n","y_pred = ab.predict(X_test)\n","print('The accuracy_score of the model is:', round(accuracy_score(y_test, y_pred), 4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuKG9djJH0ll","outputId":"845165e8-e676-469c-ceb2-e613476ab774","executionInfo":{"status":"ok","timestamp":1698845396908,"user_tz":-420,"elapsed":964,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}}},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["The accuracy_score of the model is: 0.9349\n"]}]},{"cell_type":"markdown","source":["### Using the Library Scikit-Learn implementation of AdaBoost"],"metadata":{"id":"hCEddhYbWpdP"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","ab_sk = AdaBoostClassifier(n_estimators = 50)\n","ab_sk.fit(X_train, y_train)\n","y_pred_sk = ab_sk.predict(X_test)\n","print('The accuracy_score of the model is:', round(accuracy_score(y_test, y_pred_sk), 4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0p4EIayHWruO","outputId":"5707f4f5-050c-4666-c296-a3f9270e0fe0","executionInfo":{"status":"ok","timestamp":1698845396909,"user_tz":-420,"elapsed":4,"user":{"displayName":"Hiếu Vũ","userId":"16502689377345213736"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["The accuracy_score of the model is: 0.9435\n"]}]}]}